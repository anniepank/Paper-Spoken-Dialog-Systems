\documentclass[12pt,titlepage,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{hyphenat}

\graphicspath{ {./images} }
\DeclareMathOperator*{\E}{\mathbb{E}}

\begin{document}

%===========================================================
\begin{titlepage}
\begin{center}

\textbf{\LARGE Titel der Bachelorarbeit}
%bei langen Titeln, die mehrere Zeilen benoetigen:
%\textbf{\LARGE Langer Titel der  ueber \medskip mehrere Zeilen laeuft}

\bigskip\bigskip
\textbf{Bachelorarbeit}
%bei Masterarbeit:
%\textbf{Masterarbeit}

\bigskip\bigskip\bigskip
Vorgelegt von

\bigskip
\textbf{Vorname Nachname}

\bigskip
aus Geburtsort


\vfill
Angefertigt am\\
Mathematischen Institut\\ 
der Mathematisch-Naturwissenschaftlichen Fakult\"at\\ 
der Heinrich-Heine-Universit\"at D\"usseldorf

\bigskip
%Abgabedatum:
tt.\ Monat jjjj

\bigskip
Betreuer: Prof.\ Dr.\ Vorname Nachname
%Wird der Schwerpunkt der Abschlussarbeit im Anwendungsfach gewaehlt:
%Betreuer: Prof.\ Dr.\ Vorname Nachname\\
%Zweitbetreuer: Prof.\ Dr.\ Vorname Nachname

\end{center}
\end{titlepage}

\thispagestyle{empty}\mbox{}\pagebreak
\setcounter{page}{0}

%===========================================================
\tableofcontents
\pagebreak


%===========================================================
\section*{Einleitung}
\addcontentsline{toc}{section}{Einleitung}

In der vorgelegten Abschlussarbeit untersuchen wir ...

\pagebreak
\section{Spoken Dialog System}
A spoken dialogue system is a computer system that enables human computer interaction where primary input is speech. Examples of such systems are voice assistants, chatbots, and conversational interfaces (Siri, Alexa, Google Home, etc.). \par

A good spoken dialogue system has the following properties: understands the user, decides what to say back, conducts a conversation beyond simple vo\-ice commands or question answering. 
In this overview we will be considering a goal-oriented systems (the ones which are targeted to accomplish a goal in the context of a task - to book a restaurant for ex.). In this case the scope of the conversation is usually limited to a specific domain - a database contating all knowledge about the topic. Such database is called an ontology.
\par
One of the straightforward and old approaches to create such a system is rule-base model. However such models of course require to handcraft a lot of rules, don't keep track of what has already happend in the dialog and therefore are not prone to annoying repetitive behaviour. And here machine learning approaches come in handy. \par

System is usually split into modules each responsible for a specific task with well defined input and output, for each a machine learning algorithm can be used. Spoken language understanding (SLU), Dialogue management (DM), Spoken language generation (SLG). Modular approach has a downside of loosing information between the modules, for example if we have recognised speech incorrectly all following steps will be failed, however ML approaches allow to consider a list of possible inputs scored by their probability, and therefor uncertainty is propagated through the pipeline. \par
\includegraphics[width=\linewidth]{SDS-main.png}

Both semantic decoding and natural language generation depend on the ontology. Dialogue manager in addition needs to query the ontology to be able to provide an adequate response.

\subsection{Word represeantions}
To match user utterances with the ontology concepts we either have to either handcode all of the possible user utterances (which is completely uproductive in the constantly changing language and doesn't scale well), or learn their representation implicitely. For instance using a distri\-butional approach by encoding words into vectors, which can be learned statistically from the corpus of many texts. One can learn static representation with such statistical methods as word2vec, GloVe, fastText (uses n-grams) or contextualized neural language models such as ELMo, BERT etc.

\par
With static word respresentation one can have a meaningfull distance metric, cosine similarity for ex.: the closer the word vectors are - the more similar meaning they have.

\begin{equation}
    cos(w_i,w_j) = \frac{w_i^T \cdot w_j}{||w_i|| \cdot ||w_j||}
\end{equation}

With this approach one can get examples like: $w$("queen") = $w$("king") - $w$("man") + $w$("women")
\par

\pagebreak

\section{SLU. Speech recognition}
Speech recognition might be complicated by differences in speakers, environ\-mental noises, divestiry of vocabulary and so on. Uncertanties in recognition brought by these factors shoulbe be propagated through the pipeline so that information is not lost after the SLU module because of the wrong recognition.
\begin{center}
    \includegraphics{training-1.png}
\end{center}
\par 
There is not straightforward way to convert speech into text - think for example about how to detect when user stars or ends speaking?
\par Recognition usually doesn't assume the speech being represented in words only, but it works in the hiranhical way: utterance $\Rightarrow$ words $\Rightarrow$ subwords(pho\-nemes, triphones) $\Rightarrow$ 3 senones. By subwords we understand the phonemes, by triphones that one can find the in dictionary. 3 senones represent a subwords unit - specifically its beginning, middle and end.

\begin{center}
    \includegraphics[width=\linewidth]{hierarcy-speech.png}
\end{center}

Because we have to align labels in trainging and during recognition the task is to find the most matching sequence from all possible ones we can well apply a HMM model here. Each phone is modeled with 3 states. The sequence of senones is a sequence of states for HMM. Each state of an HMM generates a feature vector of a frame. Before pure GMM were applied to solve these problems, afterwards deep learnining came into play hybrid HMM-DNN models appeared. There also exist end-to-end transformer models which train from sequence of phones and produce a sequence of frames.

\pagebreak
\section{Task 2 Semantic Decoding}
Semantic decoding translates utterances into the set of sematic concepts (dia\-log acts), which consists of 2 parts: intent (or dialog act type) and sematic slots and values. For instance the sentence "I want to book a flight to Berlin" can be translated into the intent "book" and the slots "flight" and "to" with the value "Berlin"($book(flight, to(Berlin))$).

We can view this as a classification task: having probability for each act type and each slot-value pair.
\begin{center}
    \includegraphics[width=\linewidth]{training-2.png}
\end{center}
Utterances for classification task can be represented as a fixed-size vector with the bag-of-words model. Where we preserve only information about which and how many words are there. To keep the word order information additionally one can use an n-gram model. We can also use delexicalisation to cluster special terms as <area> or <food-type> with the help of ontology in advance. Then several SVMs are trained to classify possible dialog acts (see image below).
\begin{center}
    \includegraphics[width=\linewidth]{semantic-decoding-classification.png}
\end{center}

We also consider several possible utterances (see image).
\begin{center}
    \includegraphics{uncertanty.png}
\end{center}

We can view this as a seq-to-seq task: translating an utterance into a sequence of tags, for ex. "I want to fly from Minsk to Kiev round trip tomorrow" can be translated into the sequence of tags: ["O", "O", "O", "O", "O", "B-fromloc.city\_name", "O", "B-toloc.city\_name", "B-round\_trip", "I-round\_trip", "B-departure\_date"].
\begin{center}
    \includegraphics[width=\linewidth]{training-2-2.png}
\end{center}

In order to do this, we can keep two dictionaries (one for words and one for tags), which will map each word (tag) to a unique integer. We can then use these dictionaries to translate the sequence of words and a sequence of tags into a sequence of integers. Then we can train a model to predict from the sequence of integers a corresponding sequence of tags. This can be a simple RNN with two heads - one for predicting the tags and another for predicting intent. One can also use conditional random fields (CRF) instead.

There are different ways of how we can evaluate such model. But first, having a sequence of tags, we can convert it to a set of slots in the following way: input = [B-food-type I\_food-type O B\_price-range O], output = [(food-type, start=0, end=1), (price-range, start=3, end=3)]. Then having a set of true and predicted slots we can compare them on the slot level: if predicted slot was in true slots, then this is a true positive. If predicted slot was not in true slots, then this is a false positive. If true slot was not in predicted slots, then this is a false negative. Then from these values, we can calculate precision, recall and F1 score.

On the other hand, we can calculate accuracy directly on the predicted sequence of tags, without converting it to slots. In this case we keep the mapping of each tag to 3 counters for TP, FP and FN. After that there are 2 options on how to average the metrics. On a macro level, we first calculate precision, recall and f1-score for each of the tags, then we average these metrics across all tags. On a micro level we do the weighted average, so for ex. to calculate precision we divide the sum of all TP by the sum of all TP+FP of all tags. 

Macro average can be much lower than a weighted average, because there can be a lot of rare classes with few instances which were not predicted or predicted wrongly while there are not so many classes with a lot of instances, which are therefore predicted correctly.

Let's have a look on how the tags are recognised after training. 
\begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ c c c c c c c c c c c c c }
        & find & a & flight & from & tampa & to & montreal & by & way & of & new & york
        \\ 
        predicted & O & O & O & O & B-fromloc.city\_name & O & B-toloc.city\_name & O & O & O & B-fromloc.city\_name & I-fromloc.city\_name
        \\ 
        true & O & O & O & O & B-fromloc.city\_name & O & B-toloc.city\_name & O & O & O & B-stoploc.city\_name & I-stoploc.city\_name \\  
    \end{tabular}}
\end{center}

We can fix it by adding words "with a stop in".
\begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ c c c c c c c c c c c c c c}
        & find & a & flight & from & tampa & to & montreal & with & a & stop & in & new & york
        \\ 
        predicted & O & O & O & O & B-fromloc.city\_name & O & B-toloc.city\_name & O & O & O & O & B-stoploc.city\_name & I-stoploc.city\_name  
    \end{tabular}}
\end{center}

There are also many instances when the state name is recognized as a city name. 
\begin{center}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ c c c c c c c c c c c}
        & please & find & a & flight & from & las & vegas & to & michigan
        \\ 
        predicted & O & O & O & O & O & B-fromloc.city\_name & I-fromloc.city\_name &  O & B-toloc.city\_name
        \\ 
        true & O & O & O & O & O & B-fromloc.city\_name & I-fromloc.city\_name &  O & B-toloc.state\_name \\  
    \end{tabular}}
\end{center}

Arrival time can be mistaken with departing time:
\begin{center}
    \scalebox{0.7}{
    \begin{tabular}{ c c c c c}
        Sentence & True & Predicted & Changes & New prediction\\
        i &O&O & i & O\\
        would &O&O & would & O\\
        like  &O&O & like & O\\
        to &O& O & to & O\\
        return &O& O & fly& O\\
        from &O& O & around & B-depart\_time.time\_relative\\
        chicago &B-fromloc.city\_name& B-fromloc.city\_name & 7 & B-depart\_time.time\\
        around &B-depart\_time.time\_relative& B-depart\_time.time\_relative & in & O\\
        7 &B-depart\_time.time& B-depart\_time.time & the & O\\
        pm &I-depart\_time.time& I-arrive\_time.time & evening & B-depart\_time.period\_of\_day\\
        to &O& O & from & O\\
        kansas  &B-toloc.city\_name& B-toloc.city\_name & chicago & B-fromloc.city\_name\\
        city &I-toloc.city\_name& I-toloc.city\_name & to & O \\
        &&&kansas & B-toloc.city\_name\\
        &&& city & I-toloc.city\_name\\
    \end{tabular}}
\end{center}

We can fix wrongly recognized "pm" by adding - in the evening.
%===========================================================
\section{Dialog state tracking}
Dialog state tracker remembers everything that has happend in the dialog and that is important for this dialog. Dialog management problem can be solved with a generative model with the use of Markov decision process.

\begin{center}
    \includegraphics[width=\linewidth]{training-state-tr-1.png}
\end{center}

Process is represented with a dynamic baysian network of states that contain all important information about the dialog so far, actions are what system does and rewards - measures of how good the dialog is so far.

\begin{figure}[!htb]
    \minipage{0.27\textwidth}
      \includegraphics[width=\linewidth]{markov.png}
      \caption{Markov}
    \endminipage\hfill
    \minipage{0.26\textwidth}
      \includegraphics[width=\linewidth]{partial-markov.png}
      \caption{Partial Markov}
    \endminipage\hfill
    \minipage{0.46\textwidth}
      \includegraphics[width=\linewidth]{state-split.png}
      \caption{State Split}
    \endminipage\hfill
\end{figure}

To introduce the uncertainty here, we can say that the utterances that we observe are not actual states, but that the observations (utterances) we see, and that they are dependent on the state, then the states will be only partially observed and the process is called then partially observable markov process and there is a tractable approximations for it called HIS. It works with n-best list of utterances(observed states) and decomposes a state into user goal, user action and dialog history parts.

In generative models we assume that the features between subsequent turns are conditionally independent given the underlying states, but we can model directly dependence of states on the observed features which can be correlated between the turns. This approach is called discriminative model.

The dialogue state then consists of three components - goal (for each slot a distribution over possible values), method (a distribution over possible methods), and requested slots (for each requestable slot a binary distribution - wether it was requested or not).
\par
Simple tracker might take the most probably state, but one can build a cocus tracker that accumulates evidence of a changing state over the course of the dialogue. Let's define for methods and goal components $c \in V_c$ (all possible slot values) the quantity denoting that a slot(method) is not present in turn $t$:

\begin{equation}
    \label{eqn:q_c_t}
    q_{c,t} := slu_{c,t}('none') = 1 - \sum_{v \in V_c \backslash \{'none'\}}slu_{c,t}(v)
\end{equation}

\noindent where $slu(v)$ stands for 'spoken dialog undertanding' and denotes a probability of a given slot value $v$ of a goal $c$ being the correct one.\\

\noindent Let's recursively define probabilities $p_{c,t}(v)$ for each $v$ in $V_c \backslash \{'none'\}$ as: 

\begin{align}
    \label{eqn:p_c_t}
    p_{c,t}(v) &=slu_{c,t}(v) + q_{c,t} \cdot p_{c,t-1}(v) \\
    \label{eqn:p_c_0}
    p_{c,0}(v) &= slu_{c,0}(v) \\
    \label{eqn:p_c_t_none}
    p_{c,t}('none') &= 1 - \sum_{v \in V_c \backslash \{'none'\}}p_{c,t}(v)
\end{align}

\noindent For a requestable slot $s$ we define 
\begin{align}
    \label{eqn:p_s_t}
    p_{s,t} &= slu_{s,t} + (1 - slu_{s, t})\cdot p_{s,t-1} \\
    \label{eqn:p_s_0}
    p_{s,0} &= slu_{s,0}
\end{align}

\noindent Let's prove that probabilities are well-defined, in other words that $0 \leq p_{c,t}(v) \leq 1$ $\forall v \in V_c, \forall t, \forall c$

\begin{proof}
    For goals(methods).
    Using method of mathematical induction for $t=0$: \\
    From \ref{eqn:p_c_0} since $0 \leq slu_{c,t}(v) \leq 1 \implies $
    \begin{equation}
        \begin{aligned}
            0 \leq &p_{c,0}(v) \leq 1 \\
            0 \leq &p_{c, 0}('none') \leq 1
        \end{aligned}
    \end{equation}

    \noindent Assuming that probabilities are well-defined for $t = k-1$:
    \begin{equation}
        0 \leq p_{c,k-1}(v) \leq 1
    \end{equation} 
    Since
    \begin{equation}
        \begin{aligned}
            0 \leq slu_{c,k}(v) + slu_{c,k}('none') = 1 - \sum_{w \in V_{c} \backslash \{'none', v\}} slu_{c,k}(w) \leq 1
        \end{aligned}
    \end{equation}
    and
    $ slu_{c,k}(v) \geq 0 $,  $q_{c,k} \geq 0$, $p_{c,k-1}(v) \geq 0$
    then for $t=k$ we get
    \begin{equation}
        \begin{aligned}
            0 \leq p_{c, k}(v) \overset{
                \mathrm{\ref{eqn:p_c_t}}, 
                \mathrm{\ref{eqn:q_c_t}}
                }{=} slu_{c,k}(v) + q_{c,k} \cdot p_{c, k-1}(v) \leq 1
        \end{aligned}
    \end{equation}
    As $p_{c, t}('none') \leq 1$ by definition (\ref{eqn:p_c_t_none}), we only need to prove it being $\geq 0$.\\
    
    \noindent Again by method of mathematical induction let's assume that for $t = k-1$ $p_{c, k-1}('none') \geq 0$ then for $t = k$

    \begin{equation}
        \begin{aligned}
            p_{c,k}('none') 
            &\overset{
                \mathrm{\ref{eqn:p_c_t_none}},
                \mathrm{\ref{eqn:p_c_t}}, 
                \mathrm{\ref{eqn:q_c_t}}
                }{=} 1 - \sum_{v \in V_{c} \backslash \{'none'\}} \left[ slu_{c,k}(v) + q_{c,k} \cdot p_{c,k-1}(v) \right] \\ 
            &\overset{
                \mathrm{\ref{eqn:q_c_t}}
                }{=} slu_{c,k}('none') - \sum_{v \in V_{c} \backslash \{'none'\}} slu_{c,k}('none') \cdot p_{c,k-1}(v) \\ 
            &= slu_{c,k}('none')(1 - \sum_{v \in V_{c} \backslash \{'none'\}}p_{c, k-1}(v)) 
            \\
            &\overset{\sum_{V_c}p_{c, k-1}(v) = 1}{=} slu_{c,k}('none') \cdot p_{c, k-1}('none') \geq 0
        \end{aligned}
    \end{equation}

    For requestable slots.\\
    
    Assuming that probabilities are well-defined for $t = k-1$:
    \begin{equation}
        0 \leq p_{s,k-1} \leq 1
    \end{equation}
    
    Since
    
    \begin{equation}
        0 \leq slu_{s,t} + (1 - slu_{s,t}) \leq 1
    \end{equation}

    \begin{equation}
        0 \leq p_{s,t} \overset{\ref{eqn:p_s_t}}{=} slu_{s,t} + (1 - slu_{s,t}) \cdot p_{s,t-1} \leq 1
    \end{equation}
\end{proof}

\subsection{Exercise 3 Quality Evaluation}
To evaluate the quality of a belief state tracker one can use accuracy as a fraction of turns where the top dialogue state hypothesis is correct or an l2-norm of the hypothesised distribution $p$ and the true label, where $i$ is index of the true label. 
\begin{center}
    \begin{equation}
        L_2 = (1-p_i)^2 + \sum_{j \neq i} p_j^2
    \end{equation}
\end{center}
Focus tracker performes much better for the informable slots. This is an expected behaviour as informable slot may change over time, and the probabi\-lity of the slot being informable will change over time. However for the requestable slot focus tracker doesn't show a better performance and usually the requestable slots do not change during the dialog, they are requested once and after that removed from the set of the requestable slots. The same applied for the methods, performance there is the similar to the baseline tracker as user only once "sets" by which method he wants to achieve the goal.

\subsection{Exercise 4}
What happens to a probability if a user gives the exact same utterance in
the first two turns of the dialog?

$slu = slu_{c, 1}(v) = slu_{c,2}(v) \Rightarrow q_{c,1} = q_{c, 2} = q$ for $\forall c \forall v \in V_c $

\begin{equation}
    \begin{aligned}
        &p_{c,2}(v) = slu_{c,2}(v) + q_c \cdot p_{c, 1}(v) 
        \\
        &p_{c,1}(v) = slu_{c,1}(v)
        \\
        &p_{c, 2}(v) = slu_{c,2}(v) + q_c \cdot p_{c, 1}(v) = slu(v)(1 + q_c)
    \end{aligned}
\end{equation}

$\Rightarrow$ probability of value $v$  for slot $c$ will grow $p_{c,2}(v) \geq p_{c,1}(v) \forall v \in V_c $

\begin{equation}
    \begin{aligned}
        p_{c,2}('none') = 1 - \sum_{v \in V_{c} \backslash \{'none'\}} p_{c,2}(v) \leq 1 - \sum_{v \in V_{c} \backslash \{'none'\}} p_{c,1}(v)  = p_{c,1}('none')
    \end{aligned}
\end{equation}

So the probability of value 'none' when the same utterance is repeated twice decreases.

\subsection{Exercise 5}
Let's defince another update rule:
\begin{equation}
    p_{c,t}(v) = (1-q_{c,t})\cdot slu_{c,t}(v) + q_{c,t} \cdot p_{c,t-1}(v)
\end{equation}
Assume we talked to the user for two turns and for an informable slot $c$ it holds that 
\begin{equation}
    \label{eqn:q_c_0_1}
    q_{c, 0} = 1
\end{equation}
and 
\begin{equation}
    \label{eqn:q_c_1}
    q_{c, 1} = slu_{c,1}(v) = 0.5
\end{equation}

\noindent What happens to the probability $p_{c,1}(v)$?\\

\noindent Since (\ref{eqn:q_c_0_1}) means that all probabilities were 0 for any $v$ apart from 'none' value, so $p_{c,0}(v) = 0$ for all $v \in V_c$ \\

\noindent Then $p_{c,1}(v) = (1 = 0.5) \cdot 0.5 + 0 = 0.25$ \\

\noindent As the slot was not in the first turn and then appeared in the second turn with probability 0.5, then it is okay to assume that the the probability of it might be not 0.5 (as it would be in previous update rule) but 0.25. \\

\noindent In general, such rule puts less weight on the new probabilities from current turn and more weight on the previous turn, which is also a totally reasonable update. 

\section{Better models}
It is also possible to skip semantic decoding module and predict dialog acts directly from the transcribed speech. One doesn't have to label the utterances in this case, however dexelicalisation is still needed and it is the main accuracy gainer for such methods. And it requires a semantic dictionary for all values from ontology, which is also difficult to obtain in practice.

Delexicalision can be done with word-vector embedding, as for. example similar words are used in similar contexts as well as antonyms (south, north, etc.). However contextual embedding performs even better for state tracking (when the vector embedding of the word depens on the sentence it is used in). This can be obtained with the transformer model. Self-attention layers in transformers relate different parts of the input sequence to produce a better representation for it. Multihead attention can replace delexicalisation as it compares the input sequence to possible slot values. With fine-tuned model like BERT (Bidirectional Encoder Representations from Transformers) 

\section{Dialog state tracking as a reinforcement learning problem}
Depending on the output of the state tracker, the agent can learn to perform the correct action. This is a reinforcement learning problem, where the agent is rewarded for performing the correct action. The agent can also be penalised for performing the incorrect action - like too long dialog for instance. It has the policy (its behaviour that) that it has to optimize, rewards from the environment, value function (prdiction of the future reward given current state) and model of the environment.


\subsection{Why do we define a reward?}

Reward is a measure of the success of the policy for the dialog, if we don’t have a reward, we cannot learn and define an optimal policy. With it we can define how we optimize the policy by optimizing the value of the reward. It is basically a metric for the algorithm's success. It can come form real users of from simulated ones.

\subsection{What actions can system execute?}

Interesting detail for optimizing the training process is explicitly define which actions can the system execute. This is important because we want to train the system to execute only those actions that are possible. For example, if the system is in a state where it cannot execute the action “say hello” (after user requests a restaurant's pnone number), then we should not train the system to execute that action. 

In order to implement that we can use a mask to define which actions can be executed. The mask is a binary vector of size equal to the number of actions. If the mask is 1, then the action is allowed to be executed. If the mask is 0, then the action is not allowed to be executed.

\par
Next to the value funtion there is a Q-funtion, which determines how good it is to take action $a$ in belief state $b$ and then follow a policy $\pi$?

\begin{center}
    \begin{equation}
        Q_{\pi}(b, a) = \E[R_t| b_t = b, a_t = a]
    \end{equation}
\end{center}

The optimal value of Q is:
\begin{center}
    \begin{align}
        &V_{*}(b) = max_{\pi}V_{\pi}(b) \\
        &Q_{*}(b, a) = max_{\pi}Q_{\pi}(b, a) = \E[r_{t+1} + \gamma * V_{*}(b_{t+1})| b_t = b, a_t = a] \\
    \end{align}
\end{center}

Bellman Optimality equation for Q-function:
\begin{center}
    \begin{equation}
        Q_{*}(b, a) = \sum_{b^\prime, r} p(b^\prime, r | b, a)[r + \gamma * max_{a^\prime} Q_{*}(b^\prime, a^\prime)]
    \end{equation}
\end{center} 

Then optimal policy is 
\begin{center}
    \begin{equation}
        \pi_{*}(b) = argmax_{a} Q_{*}(b, a)
    \end{equation}
\end{center} 

\subsection{Monte-Carlo policy}

Monte Carlo control algorithm is a method for finding the optimal policy for a reinforcement problem.

It is a tabular approach, which means that the belief state/summary space is discretised into a grid. In this algorithm the policy and the Q function are first defined randomly. Then we iterate over the dialogs with this polilcy. Throught this iteration we calculate the reward until the end of the dialog. For updating the state we choose the nearest state from the table and update the Q value and afterwards the policy with it, then in the next turn we will use an already updated policy. 

\begin{center}
    \includegraphics[width=10cm]{MCC.png}
\end{center}
It is a value-based algorithm, which means that it is based on the value of the state-action pairs. It is also a model-free method as it does not require any knowledge of the environment. And finally it is a on-policy as we use the current best estimate of the policy to guide the agent. And policy is updated iteratively until we find the optimal one. Regarding the exploration, it is a greedy method as we sometimes choose the action with the highest value, and sometimes pick a random one. Distance between distributions is measured with Jensen-Shannon divergence, which is defined in the following way:


\pagebreak


%===========================================================
\section{Deep Q-Network}

%===========================================================
\subsection{Q-Learning}
Instead of keeping Q-values in a table, one can approximate a Q-function with a neural network (Q-network). It is a neural network that takes a state of a dialog as input and outputs Q-values for each possible action. An optimal Q-network obeys the following Bellman optimality equation: 

\[ Q^*(s,a) = \E [r + \gamma max_{a^\prime}Q(s^\prime,a^\prime)] \] 

where $\gamma$ is a discount factor,  $r$ is an immediate reward and $\max_a Q(s',a)$ is the maximum Q-value of the next state. One uses Bellman equation to update the Q-function iteratively:

\[Q_{i + 1} (s,a) \leftarrow \E[r + \gamma max_{a^\prime}Q_{i}(s^\prime,a^\prime)]\]

With iterative updates it will converge to an optimal Q-function.

In comparison to Monte-Carlo this approach this one is more efficient as it does not keep track of all the past states. Instead it only remembers the current one. This happens because only Q-value of the current state is used to calculate a Q-value of the next state. Monte-Carlo reaches the maximum success rate of TODO 76\% close to the 2000 dialogs, whereas Q-learning reaches the maximum success rate of 96\% after 2000 dialogs and has already 94\% after ~400 dialogs. The reward has almost doubled as well. 

Two reasons for benefits of DQN over Monte-Caro are: experience replay and delayed Q-targets. For experience replay one stores tuples ${s, a, r, s^\prime}$ in the buffer, where $s^\prime$ is the following dialog state and samples batches from the it. Delayed targets allow to stabilize the network as we the target network’s parameters are not trained, but they are periodically synchronized with the parameters of the main Q-network. The loss funciton here is the difference between Q-value of the current state + current reward vs. Q-value of the next state:

\[\L = \E_{(s, a, r, s^\prime) \sim D} [(Q_{target}(r,s^\prime) - Q(s, a))^2]\]

where $Q_{target}(r, s^\prime) = r + \gamma max_{a}Q_{target}(s^\prime,a)$

if $s^\prime$ is a terminal state, then $Q(s^\prime, a) = Q_{target}(s^\prime, a) = 0$ for every $a$.

Additionally, we have trained a Q-network without experience replay. In this case reward was almost 3 times lower TODO (11.23 and 4.5) as well as the success rate. The variance has also increased and therefore the network became more unstable. One can see the results in Table TODO.

\begin{center}
    \begin{tabular}{||c c c c||} 
     \hline
      & Reward & Success & Turns \\ [0.5ex] 
     \hline\hline
     Experience replay & TODO 11.23 +- 0.97 & 93.00 +- 3.56 & 7.37 +- 0.53 \\ 
     \hline
     No experience replay & 4.50 +- 2.04 & 69.00 +- 6.45 & 9.30 +- 0.94 \\
     \hline
    \end{tabular}
\end{center}

The same informations is presente also in the plots.

TODO change images
\begin{figure}[!htb]
    \minipage{0.32\textwidth}
      \includegraphics[width=\linewidth]{env2-CamRestaurants-experience.png}
      \caption{Experience Replay}
    \endminipage\hfill
    \minipage{0.32\textwidth}
      \includegraphics[width=\linewidth]{env2-CamRestaurants-experience.png}
      \caption{No Experience Replay}
    \endminipage\hfill
    \minipage{0.32\textwidth}%
      \includegraphics[width=\linewidth]{env2-CamRestaurants-mc.png}
      \caption{Monte-Carlo}
    \endminipage
\end{figure}

Here are several dialogs conducted with the system: 

\begin{center}
    \includegraphics[width=\linewidth]{wants_food_type.png}
\end{center}

\begin{center}
    \includegraphics[width=\linewidth]{name\ !=.png}
\end{center}

One can see that system sometimes is caught in the loop and keeps asking the same question over and over again. The reason might be that semantic decoder didn't work well and haven't recognised an imput correctly, so the systems continue to request more information. TODO get example

It also noticable that system doesn't work well with answers like "I don't care" or "It doesn't matter" and insists on user making a choice (food-type for ex. see Figure TODO).

Somtimes it tries to inform on something else instead of providing a phone number. It might be the fault of the semantic decoder though, as the system decides that utterance "phone" is not a phone number request, but a request to find another restaurant. It might be that longer more human-like sentences "And what is the address of the suggested restaurant?" are more suitable.

There was no instances in the dataset where the user would say "I don't care" regarding the price range, and we can see an example of how the dialog is ruined when we encounter such a case, compare dialog 1 vs dialog 2. TODO dialog number Usually such infinite loops may happen due to bad recognition of speech, however here the problem is hidden probably in the state tracker. 


\begin{figure}[!htb]
    \minipage{0.49\textwidth}
      \includegraphics[width=\linewidth]{with_price.png}
      \caption{With price}
    \endminipage\hfill
    \minipage{0.49\textwidth}%
      \includegraphics[width=\linewidth]{without_price.png}
      \caption{Without price}
    \endminipage
\end{figure}

Another noticable thing is that choice of value for one slot sometimes influences the dialog drastically even though the slot is not that relevant. For example, here in the dialog 3 we choose specific price in comparison with dialog 4 TODO check number where we are agreed to any price and that leads suddenly to an infinite loop. This might happen due to insufficient semantic decoder knowledge and not enough of such samples in the database.
\begin{figure}[!htb]
    \minipage{0.49\textwidth}
      \includegraphics[width=\linewidth]{chinese_works.png}
      \caption{Working sample}
    \endminipage\hfill
    \minipage{0.49\textwidth}%
      \includegraphics[width=\linewidth]{chinese_no_work.png}
      \caption{Not working sample}
    \endminipage
\end{figure}

Examples with negations are performing poorly as well which is also most probably the failure of the semantic decoder (see dialog 5 TODO change).

\begin{figure}
    \includegraphics[width=5cm]{chinese\ negation.png}
\end{figure}


\section{Information Gain}

One can also substitute overall reward $r$ with $r = r + \nu r_i$, here $r_i$ is an additional intrinsic reward, and $\nu$ is a hyperparameter. This reward is called information gain and it measures how much additional information the system gets about user needs after one turn. Let action be of form $\{request, confirm,$ $select\}\_\xi$. Let $p_{\xi}$ and ${p_{\xi}}^\prime$ be belief distributions over possible values for $\xi$ in current and next belief state. Then the informa\-tion gain is the difference between these distributions with Jensen-Shannon divergence as a distance metric:

\begin{equation}
    r_i(s, a, s\prime) =
    \begin{cases}
        d(p_{\xi}, p_{\xi}^\prime) \\
        0, \text{if } a \notin \text{\{request, confirm, select\}}
    \end{cases}
\end{equation}
where
\begin{equation}
    \begin{aligned}
        &d(p, q) = JS(p,q) = \frac{1}{2}(KL[p||m] + KL[q||m]), \\
        &m = \frac{1}{2}(p + q)
    \end{aligned}
\end{equation}

When the distance is big enough (higher than a threshold) we add the information gain to a reward additionally. Reward will be added only when the action was of the form ${request, confirm, select}$. Intuitively it also doesn't make sense to calculate information gain for actions like $bye, inform$ as they do not provide any additonal information on user needs and therefore do not lead to the accomplishment of the dialog goal. It is very beneficial to use intrinsic rewards in addition as they encourage system to get as much useful information from users as possible, by requesting more information from them, selecting the most relevant one and confirming the choices.

Here one can see the difference in training with information gain and without it. Information gain improves the speed of training in the beginning and slightly improves the overall performance from $96\%$  to $98.4\%$ (see Figure 7, 8).

\begin{figure}[!htb]
    \minipage{0.5\textwidth}
      \includegraphics[width=\linewidth]{env3-ig-CamRestaurants.png}
      \caption{Information Gain}
    \endminipage\hfill
    \minipage{0.5\textwidth}
      \includegraphics[width=\linewidth]{env3-CamRestaurants.png}
      \caption{No Information Gain}
    \endminipage
\end{figure}

One can also substitute $r = r + \nu r_i$. However one should be cautious --- if the value $\mu$ is too high, we push the sytem to gather more and more information and discourage it from making a choice and informing a user about it. 

%=========================================================== 
\section{Zweiter Abschnitt}

However, these methods were previously quite unstable (Mnih et al., 2013). In DQN,
Mnih et al. (2013, 2015) proposed two techniques
to overcome this instability-namely experience replay and the use of a target network. In experience replay, all the transitions are put in a finite
pool D (Lin, 1993). Once the pool has reached
its predefined maximum size, adding a new transition results in deleting the oldest transition in
the pool. During training, a mini-batch of transitions is uniformly sampled from the pool, i.e.
(Bt
, At
, Rt+1, Bt+1) from U(D). This method removes the instability arising from strong correlation between the subsequent transitions of an
episode (a dialogue). Additionally, a target network with weight vector w is used. This target
network is similar to the Q-network except that
its weights are only copied every tau steps from the
Q-network, and remain fixed during all the other
steps. The loss function for the Q-network at iter

\pagebreak
\begin{thebibliography}{ccccc}
\addcontentsline{toc}{section}{Literatur}

%So kann eine eine Monographie zitieren:
\bibitem{Arbarello et al 1985}
E.\ Arbarello, M.\ Cornalba, P.\ Griffiths, J.\ Harris:
Geometry of algebraic curves. I. 
Springer, New York, 1985.

%So kann man eine Originalarbeit zitieren:
\bibitem{Shepherd-Barron 1997}
N.\ Shepherd-Barron:
Fano threefolds in positive characteristic.
Compositio Math.\  105  (1997),  237--265.

\end{thebibliography}



%===========================================================
\pagebreak\noindent
\textbf{\LARGE Erkl\"arung}
\addcontentsline{toc}{section}{Erkl\"arung}

\bigskip\bigskip
\noindent 
Hiermit versichere ich, dass ich die   Bachelorarbeit selbst\"andig verfasst und keine
anderen als die angegebenen Quellen und Hilfsmittel benutzt habe.

\bigskip
\noindent
D\"usseldorf, den tt.\ Monat jjjj

\bigskip\bigskip\bigskip
\noindent
(Vorname Nachname)

\end{document}